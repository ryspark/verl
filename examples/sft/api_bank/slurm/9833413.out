W0304 00:39:47.669000 22696596451968 torch/distributed/run.py:779] 
W0304 00:39:47.669000 22696596451968 torch/distributed/run.py:779] *****************************************
W0304 00:39:47.669000 22696596451968 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0304 00:39:47.669000 22696596451968 torch/distributed/run.py:779] *****************************************
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/iris/u/rypark/code/verl/verl/utils/tokenizer.py:29: UserWarning: tokenizer.pad_token_id is None. Now set to 128009
  warnings.warn(f'tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}')
Normalize batch size by dp 4
Using sequence parallel size: 1
Using remove padding: False
Using FSDP rank 0 and size 4 for data distribution
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/iris/u/rypark/code/verl/verl/utils/tokenizer.py:29: UserWarning: tokenizer.pad_token_id is None. Now set to 128009
  warnings.warn(f'tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}')
/iris/u/rypark/code/verl/verl/utils/tokenizer.py:29: UserWarning: tokenizer.pad_token_id is None. Now set to 128009
  warnings.warn(f'tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}')
/iris/u/rypark/code/verl/verl/utils/tokenizer.py:29: UserWarning: tokenizer.pad_token_id is None. Now set to 128009
  warnings.warn(f'tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}')
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.15s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]
functools.partial(<function _or_policy at 0x1541807043a0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x154180704280>, transformer_layer_cls={<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>})])
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]
NCCL version 2.20.5+cuda12.4
Total training steps: 54
Total training steps: 54
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
Number of steps/epoch 54, number of epochs 1, total number of steps 54
{'data': {'train_batch_size': 64, 'micro_batch_size': None, 'micro_batch_size_per_gpu': 1, 'train_files': '/iris/u/rypark/code/dense-tool-rewards/data/train_only_api.parquet', 'val_files': '/iris/u/rypark/code/dense-tool-rewards/data/test_only_api.parquet', 'pretemplated': True, 'prompt_key': 'input', 'response_key': 'output', 'max_length': 4096, 'truncation': 'error', 'balance_dp_token': False, 'chat_template': None}, 'model': {'partial_pretrain': 'meta-llama/Llama-3.2-3B-Instruct', 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'cpu_offload': False, 'offload_params': False}, 'external_lib': None, 'enable_gradient_checkpointing': False, 'trust_remote_code': False, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear', 'use_liger': False}, 'optim': {'lr': 1e-05, 'betas': [0.9, 0.95], 'weight_decay': 0.01, 'warmup_steps_ratio': 0.1, 'clip_grad': 1.0}, 'ulysses_sequence_parallel_size': 1, 'use_remove_padding': False, 'trainer': {'default_local_dir': '/iris/u/rypark/code/dense-tool-rewards/models/llama_32_3b_instruct_l1l2_onlyapi', 'default_hdfs_dir': None, 'resume_path': None, 'project_name': 'dense-tool-rewards', 'experiment_name': 'llama-3.2-3b-instruct-l1l2-onlyapi', 'total_epochs': 1, 'total_training_steps': None, 'logger': ['console', 'wandb'], 'seed': 1}}
Total training steps: 54
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: orangese to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.7
wandb: Run data is saved locally in /iris/u/rypark/code/verl/examples/sft/api_bank/wandb/run-20250304_004013-f80bvhbs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama-3.2-3b-instruct-l1l2-onlyapi
wandb: ⭐️ View project at https://wandb.ai/orangese/dense-tool-rewards
wandb: 🚀 View run at https://wandb.ai/orangese/dense-tool-rewards/runs/f80bvhbs
Using LocalLogger is deprecated. The constructor API will change 
Total training steps: 54
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
Epoch 1/1:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 1/1:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 1/1:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 1/1:   0%|          | 0/54 [00:00<?, ?it/s]/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch 1/1:   2%|▏         | 1/54 [00:37<33:15, 37.65s/it]Epoch 1/1:   2%|▏         | 1/54 [00:37<33:15, 37.65s/it]Epoch 1/1:   2%|▏         | 1/54 [00:37<33:15, 37.65s/it]step:0 - train/loss:0.801 - train/lr(1e-3):0.002
Epoch 1/1:   2%|▏         | 1/54 [00:31<27:37, 31.28s/it]Epoch 1/1:   4%|▎         | 2/54 [01:01<25:43, 29.69s/it]Epoch 1/1:   4%|▎         | 2/54 [01:01<25:43, 29.69s/it]Epoch 1/1:   4%|▎         | 2/54 [01:01<25:43, 29.69s/it]step:1 - train/loss:0.824 - train/lr(1e-3):0.004
Epoch 1/1:   4%|▎         | 2/54 [00:55<23:26, 27.05s/it]Epoch 1/1:   6%|▌         | 3/54 [01:26<23:10, 27.26s/it]Epoch 1/1:   6%|▌         | 3/54 [01:26<23:10, 27.26s/it]Epoch 1/1:   6%|▌         | 3/54 [01:26<23:10, 27.26s/it]step:2 - train/loss:0.770 - train/lr(1e-3):0.006
Epoch 1/1:   6%|▌         | 3/54 [01:19<21:57, 25.83s/it]Epoch 1/1:   7%|▋         | 4/54 [01:50<21:52, 26.25s/it]Epoch 1/1:   7%|▋         | 4/54 [01:50<21:52, 26.25s/it]Epoch 1/1:   7%|▋         | 4/54 [01:50<21:52, 26.25s/it]step:3 - train/loss:0.581 - train/lr(1e-3):0.008
Epoch 1/1:   7%|▋         | 4/54 [01:44<21:09, 25.39s/it]step:4 - train/loss:0.443 - train/lr(1e-3):0.010
Epoch 1/1:   9%|▉         | 5/54 [02:14<20:36, 25.22s/it]Epoch 1/1:   9%|▉         | 5/54 [02:14<20:36, 25.22s/it]Epoch 1/1:   9%|▉         | 5/54 [02:07<20:08, 24.67s/it]Epoch 1/1:   9%|▉         | 5/54 [02:14<20:36, 25.22s/it]Epoch 1/1:  11%|█         | 6/54 [02:38<19:50, 24.80s/it]Epoch 1/1:  11%|█         | 6/54 [02:38<19:50, 24.80s/it]Epoch 1/1:  11%|█         | 6/54 [02:38<19:50, 24.80s/it]step:5 - train/loss:0.388 - train/lr(1e-3):0.010
Epoch 1/1:  11%|█         | 6/54 [02:31<19:32, 24.43s/it]Epoch 1/1:  13%|█▎        | 7/54 [03:03<19:33, 24.97s/it]Epoch 1/1:  13%|█▎        | 7/54 [03:03<19:33, 24.97s/it]step:6 - train/loss:0.339 - train/lr(1e-3):0.010Epoch 1/1:  13%|█▎        | 7/54 [03:03<19:33, 24.97s/it]
Epoch 1/1:  13%|█▎        | 7/54 [02:57<19:22, 24.73s/it]Epoch 1/1:  15%|█▍        | 8/54 [03:27<18:57, 24.73s/it]Epoch 1/1:  15%|█▍        | 8/54 [03:27<18:57, 24.73s/it]step:7 - train/loss:0.333 - train/lr(1e-3):0.010
Epoch 1/1:  15%|█▍        | 8/54 [03:21<18:49, 24.56s/it]Epoch 1/1:  15%|█▍        | 8/54 [03:27<18:57, 24.73s/it]step:8 - train/loss:0.272 - train/lr(1e-3):0.010
Epoch 1/1:  17%|█▋        | 9/54 [03:52<18:26, 24.60s/it]Epoch 1/1:  17%|█▋        | 9/54 [03:52<18:26, 24.60s/it]Epoch 1/1:  17%|█▋        | 9/54 [03:52<18:26, 24.60s/it]Epoch 1/1:  17%|█▋        | 9/54 [03:45<18:21, 24.48s/it]step:9 - train/loss:0.207 - train/lr(1e-3):0.010
Epoch 1/1:  19%|█▊        | 10/54 [04:16<17:55, 24.45s/it]Epoch 1/1:  19%|█▊        | 10/54 [04:09<17:52, 24.37s/it]Epoch 1/1:  19%|█▊        | 10/54 [04:16<17:55, 24.45s/it]Epoch 1/1:  19%|█▊        | 10/54 [04:16<17:55, 24.45s/it]Epoch 1/1:  20%|██        | 11/54 [04:39<17:15, 24.09s/it]Epoch 1/1:  20%|██        | 11/54 [04:39<17:15, 24.09s/it]step:10 - train/loss:0.210 - train/lr(1e-3):0.010Epoch 1/1:  20%|██        | 11/54 [04:39<17:15, 24.09s/it]
Epoch 1/1:  20%|██        | 11/54 [04:33<17:13, 24.03s/it]step:11 - train/loss:0.234 - train/lr(1e-3):0.010Epoch 1/1:  22%|██▏       | 12/54 [05:04<17:00, 24.30s/it]
Epoch 1/1:  22%|██▏       | 12/54 [05:04<17:00, 24.30s/it]Epoch 1/1:  22%|██▏       | 12/54 [05:04<17:00, 24.30s/it]Epoch 1/1:  22%|██▏       | 12/54 [04:57<16:59, 24.26s/it]Epoch 1/1:  24%|██▍       | 13/54 [05:28<16:35, 24.29s/it]Epoch 1/1:  24%|██▍       | 13/54 [05:28<16:35, 24.29s/it]Epoch 1/1:  24%|██▍       | 13/54 [05:28<16:35, 24.29s/it]step:12 - train/loss:0.212 - train/lr(1e-3):0.009
Epoch 1/1:  24%|██▍       | 13/54 [05:22<16:34, 24.26s/it]Epoch 1/1:  26%|██▌       | 14/54 [05:53<16:18, 24.45s/it]Epoch 1/1:  26%|██▌       | 14/54 [05:53<16:18, 24.45s/it]Epoch 1/1:  26%|██▌       | 14/54 [05:53<16:18, 24.45s/it]step:13 - train/loss:0.193 - train/lr(1e-3):0.009
Epoch 1/1:  26%|██▌       | 14/54 [05:46<16:17, 24.43s/it]step:14 - train/loss:0.210 - train/lr(1e-3):0.009
Epoch 1/1:  28%|██▊       | 15/54 [06:17<15:54, 24.47s/it]Epoch 1/1:  28%|██▊       | 15/54 [06:17<15:54, 24.47s/it]Epoch 1/1:  28%|██▊       | 15/54 [06:17<15:54, 24.47s/it]Epoch 1/1:  28%|██▊       | 15/54 [06:11<15:53, 24.46s/it]Epoch 1/1:  30%|██▉       | 16/54 [06:40<15:11, 24.00s/it]Epoch 1/1:  30%|██▉       | 16/54 [06:40<15:11, 24.00s/it]Epoch 1/1:  30%|██▉       | 16/54 [06:40<15:11, 24.00s/it]step:15 - train/loss:0.185 - train/lr(1e-3):0.009
Epoch 1/1:  30%|██▉       | 16/54 [06:34<15:11, 23.99s/it]Epoch 1/1:  31%|███▏      | 17/54 [07:05<14:57, 24.25s/it]Epoch 1/1:  31%|███▏      | 17/54 [07:05<14:57, 24.25s/it]step:16 - train/loss:0.180 - train/lr(1e-3):0.009Epoch 1/1:  31%|███▏      | 17/54 [07:05<14:57, 24.25s/it]
Epoch 1/1:  31%|███▏      | 17/54 [06:59<14:57, 24.26s/it]Epoch 1/1:  33%|███▎      | 18/54 [07:30<14:37, 24.37s/it]Epoch 1/1:  33%|███▎      | 18/54 [07:30<14:37, 24.37s/it]Epoch 1/1:  33%|███▎      | 18/54 [07:30<14:37, 24.37s/it]step:17 - train/loss:0.137 - train/lr(1e-3):0.008
Epoch 1/1:  33%|███▎      | 18/54 [07:23<14:37, 24.36s/it]Epoch 1/1:  35%|███▌      | 19/54 [07:54<14:16, 24.48s/it]Epoch 1/1:  35%|███▌      | 19/54 [07:54<14:16, 24.48s/it]step:18 - train/loss:0.156 - train/lr(1e-3):0.008Epoch 1/1:  35%|███▌      | 19/54 [07:54<14:16, 24.48s/it]
Epoch 1/1:  35%|███▌      | 19/54 [07:48<14:16, 24.47s/it]step:19 - train/loss:0.154 - train/lr(1e-3):0.008Epoch 1/1:  37%|███▋      | 20/54 [08:19<13:54, 24.54s/it]Epoch 1/1:  37%|███▋      | 20/54 [08:19<13:54, 24.54s/it]Epoch 1/1:  37%|███▋      | 20/54 [08:19<13:54, 24.54s/it]
Epoch 1/1:  37%|███▋      | 20/54 [08:13<13:54, 24.53s/it]Epoch 1/1:  39%|███▉      | 21/54 [08:42<13:12, 24.00s/it]step:20 - train/loss:0.147 - train/lr(1e-3):0.008Epoch 1/1:  39%|███▉      | 21/54 [08:42<13:12, 24.00s/it]Epoch 1/1:  39%|███▉      | 21/54 [08:42<13:12, 24.00s/it]
Epoch 1/1:  39%|███▉      | 21/54 [08:35<13:12, 24.00s/it]step:21 - train/loss:0.179 - train/lr(1e-3):0.007
Epoch 1/1:  41%|████      | 22/54 [09:07<12:54, 24.21s/it]Epoch 1/1:  41%|████      | 22/54 [09:07<12:54, 24.21s/it]Epoch 1/1:  41%|████      | 22/54 [09:07<12:54, 24.21s/it]Epoch 1/1:  41%|████      | 22/54 [09:00<12:54, 24.21s/it]Epoch 1/1:  43%|████▎     | 23/54 [09:31<12:29, 24.18s/it]step:22 - train/loss:0.168 - train/lr(1e-3):0.007Epoch 1/1:  43%|████▎     | 23/54 [09:31<12:29, 24.18s/it]
Epoch 1/1:  43%|████▎     | 23/54 [09:31<12:29, 24.18s/it]Epoch 1/1:  43%|████▎     | 23/54 [09:24<12:29, 24.18s/it]Epoch 1/1:  44%|████▍     | 24/54 [09:56<12:11, 24.38s/it]step:23 - train/loss:0.163 - train/lr(1e-3):0.007Epoch 1/1:  44%|████▍     | 24/54 [09:56<12:11, 24.38s/it]Epoch 1/1:  44%|████▍     | 24/54 [09:56<12:11, 24.38s/it]
Epoch 1/1:  44%|████▍     | 24/54 [09:49<12:11, 24.38s/it]step:24 - train/loss:0.148 - train/lr(1e-3):0.006
Epoch 1/1:  46%|████▋     | 25/54 [10:20<11:49, 24.48s/it]Epoch 1/1:  46%|████▋     | 25/54 [10:20<11:49, 24.48s/it]Epoch 1/1:  46%|████▋     | 25/54 [10:20<11:49, 24.48s/it]Epoch 1/1:  46%|████▋     | 25/54 [10:14<11:49, 24.48s/it]step:25 - train/loss:0.157 - train/lr(1e-3):0.006
Epoch 1/1:  48%|████▊     | 26/54 [10:43<11:14, 24.08s/it]Epoch 1/1:  48%|████▊     | 26/54 [10:43<11:14, 24.08s/it]Epoch 1/1:  48%|████▊     | 26/54 [10:43<11:14, 24.08s/it]Epoch 1/1:  48%|████▊     | 26/54 [10:37<11:14, 24.08s/it]step:26 - train/loss:0.152 - train/lr(1e-3):0.006
Epoch 1/1:  50%|█████     | 27/54 [11:08<10:52, 24.18s/it]Epoch 1/1:  50%|█████     | 27/54 [11:08<10:52, 24.18s/it]Epoch 1/1:  50%|█████     | 27/54 [11:08<10:52, 24.18s/it]Epoch 1/1:  50%|█████     | 27/54 [11:01<10:52, 24.18s/it]step:27 - train/loss:0.140 - train/lr(1e-3):0.005
Epoch 1/1:  52%|█████▏    | 28/54 [11:33<10:34, 24.38s/it]Epoch 1/1:  52%|█████▏    | 28/54 [11:33<10:34, 24.38s/it]Epoch 1/1:  52%|█████▏    | 28/54 [11:33<10:34, 24.38s/it]Epoch 1/1:  52%|█████▏    | 28/54 [11:26<10:34, 24.38s/it]Epoch 1/1:  54%|█████▎    | 29/54 [11:57<10:12, 24.48s/it]Epoch 1/1:  54%|█████▎    | 29/54 [11:57<10:12, 24.48s/it]Epoch 1/1:  54%|█████▎    | 29/54 [11:57<10:12, 24.48s/it]step:28 - train/loss:0.119 - train/lr(1e-3):0.005
Epoch 1/1:  54%|█████▎    | 29/54 [11:51<10:12, 24.48s/it]Epoch 1/1:  56%|█████▌    | 30/54 [12:22<09:49, 24.54s/it]Epoch 1/1:  56%|█████▌    | 30/54 [12:22<09:49, 24.54s/it]Epoch 1/1:  56%|█████▌    | 30/54 [12:22<09:49, 24.54s/it]step:29 - train/loss:0.181 - train/lr(1e-3):0.005
Epoch 1/1:  56%|█████▌    | 30/54 [12:16<09:49, 24.54s/it]Epoch 1/1:  57%|█████▋    | 31/54 [12:47<09:25, 24.57s/it]Epoch 1/1:  57%|█████▋    | 31/54 [12:47<09:25, 24.57s/it]Epoch 1/1:  57%|█████▋    | 31/54 [12:47<09:25, 24.57s/it]step:30 - train/loss:0.127 - train/lr(1e-3):0.005
Epoch 1/1:  57%|█████▋    | 31/54 [12:40<09:25, 24.57s/it]Epoch 1/1:  59%|█████▉    | 32/54 [13:10<08:49, 24.05s/it]Epoch 1/1:  59%|█████▉    | 32/54 [13:10<08:49, 24.05s/it]step:31 - train/loss:0.138 - train/lr(1e-3):0.004Epoch 1/1:  59%|█████▉    | 32/54 [13:10<08:49, 24.05s/it]
Epoch 1/1:  59%|█████▉    | 32/54 [13:03<08:49, 24.05s/it]step:32 - train/loss:0.126 - train/lr(1e-3):0.004Epoch 1/1:  61%|██████    | 33/54 [13:34<08:28, 24.23s/it]
Epoch 1/1:  61%|██████    | 33/54 [13:34<08:28, 24.23s/it]Epoch 1/1:  61%|██████    | 33/54 [13:34<08:28, 24.23s/it]Epoch 1/1:  61%|██████    | 33/54 [13:28<08:28, 24.23s/it]Epoch 1/1:  63%|██████▎   | 34/54 [13:59<08:06, 24.33s/it]step:33 - train/loss:0.166 - train/lr(1e-3):0.004Epoch 1/1:  63%|██████▎   | 34/54 [13:59<08:06, 24.33s/it]Epoch 1/1:  63%|██████▎   | 34/54 [13:59<08:06, 24.33s/it]
Epoch 1/1:  63%|██████▎   | 34/54 [13:52<08:06, 24.33s/it]step:34 - train/loss:0.133 - train/lr(1e-3):0.003Epoch 1/1:  65%|██████▍   | 35/54 [14:23<07:43, 24.39s/it]
Epoch 1/1:  65%|██████▍   | 35/54 [14:23<07:43, 24.39s/it]Epoch 1/1:  65%|██████▍   | 35/54 [14:23<07:43, 24.39s/it]Epoch 1/1:  65%|██████▍   | 35/54 [14:17<07:43, 24.39s/it]step:35 - train/loss:0.139 - train/lr(1e-3):0.003Epoch 1/1:  67%|██████▋   | 36/54 [14:48<07:21, 24.53s/it]Epoch 1/1:  67%|██████▋   | 36/54 [14:48<07:21, 24.53s/it]Epoch 1/1:  67%|██████▋   | 36/54 [14:48<07:21, 24.53s/it]
Epoch 1/1:  67%|██████▋   | 36/54 [14:42<07:21, 24.53s/it]Epoch 1/1:  69%|██████▊   | 37/54 [15:11<06:50, 24.15s/it]Epoch 1/1:  69%|██████▊   | 37/54 [15:11<06:50, 24.15s/it]Epoch 1/1:  69%|██████▊   | 37/54 [15:11<06:50, 24.15s/it]step:36 - train/loss:0.136 - train/lr(1e-3):0.003
Epoch 1/1:  69%|██████▊   | 37/54 [15:05<06:50, 24.15s/it]Epoch 1/1:  70%|███████   | 38/54 [15:36<06:28, 24.30s/it]Epoch 1/1:  70%|███████   | 38/54 [15:36<06:28, 24.30s/it]Epoch 1/1:  70%|███████   | 38/54 [15:36<06:28, 24.30s/it]step:37 - train/loss:0.130 - train/lr(1e-3):0.002
Epoch 1/1:  70%|███████   | 38/54 [15:30<06:28, 24.30s/it]step:38 - train/loss:0.126 - train/lr(1e-3):0.002
Epoch 1/1:  72%|███████▏  | 39/54 [16:00<06:04, 24.28s/it]Epoch 1/1:  72%|███████▏  | 39/54 [16:00<06:04, 24.28s/it]Epoch 1/1:  72%|███████▏  | 39/54 [16:00<06:04, 24.28s/it]Epoch 1/1:  72%|███████▏  | 39/54 [15:54<06:04, 24.28s/it]Epoch 1/1:  74%|███████▍  | 40/54 [16:25<05:40, 24.34s/it]Epoch 1/1:  74%|███████▍  | 40/54 [16:25<05:40, 24.34s/it]Epoch 1/1:  74%|███████▍  | 40/54 [16:25<05:40, 24.34s/it]step:39 - train/loss:0.104 - train/lr(1e-3):0.002
Epoch 1/1:  74%|███████▍  | 40/54 [16:18<05:40, 24.34s/it]step:40 - train/loss:0.143 - train/lr(1e-3):0.002Epoch 1/1:  76%|███████▌  | 41/54 [16:49<05:15, 24.28s/it]Epoch 1/1:  76%|███████▌  | 41/54 [16:49<05:15, 24.28s/it]Epoch 1/1:  76%|███████▌  | 41/54 [16:49<05:15, 24.28s/it]
Epoch 1/1:  76%|███████▌  | 41/54 [16:43<05:15, 24.28s/it]Epoch 1/1:  78%|███████▊  | 42/54 [17:11<04:44, 23.74s/it]Epoch 1/1:  78%|███████▊  | 42/54 [17:11<04:44, 23.74s/it]Epoch 1/1:  78%|███████▊  | 42/54 [17:11<04:44, 23.74s/it]step:41 - train/loss:0.146 - train/lr(1e-3):0.001
Epoch 1/1:  78%|███████▊  | 42/54 [17:05<04:44, 23.74s/it]Epoch 1/1:  80%|███████▉  | 43/54 [17:36<04:22, 23.90s/it]Epoch 1/1:  80%|███████▉  | 43/54 [17:36<04:22, 23.90s/it]Epoch 1/1:  80%|███████▉  | 43/54 [17:36<04:22, 23.90s/it]step:42 - train/loss:0.124 - train/lr(1e-3):0.001
Epoch 1/1:  80%|███████▉  | 43/54 [17:29<04:22, 23.90s/it]step:43 - train/loss:0.120 - train/lr(1e-3):0.001
Epoch 1/1:  81%|████████▏ | 44/54 [18:00<04:00, 24.05s/it]Epoch 1/1:  81%|████████▏ | 44/54 [18:00<04:00, 24.05s/it]Epoch 1/1:  81%|████████▏ | 44/54 [18:00<04:00, 24.05s/it]Epoch 1/1:  81%|████████▏ | 44/54 [17:54<04:00, 24.05s/it]Epoch 1/1:  83%|████████▎ | 45/54 [18:25<03:37, 24.17s/it]Epoch 1/1:  83%|████████▎ | 45/54 [18:25<03:37, 24.17s/it]Epoch 1/1:  83%|████████▎ | 45/54 [18:25<03:37, 24.17s/it]step:44 - train/loss:0.148 - train/lr(1e-3):0.001
Epoch 1/1:  83%|████████▎ | 45/54 [18:18<03:37, 24.17s/it]step:45 - train/loss:0.132 - train/lr(1e-3):0.001
Epoch 1/1:  85%|████████▌ | 46/54 [18:48<03:12, 24.10s/it]Epoch 1/1:  85%|████████▌ | 46/54 [18:48<03:12, 24.10s/it]Epoch 1/1:  85%|████████▌ | 46/54 [18:48<03:12, 24.10s/it]Epoch 1/1:  85%|████████▌ | 46/54 [18:42<03:12, 24.10s/it]step:46 - train/loss:0.117 - train/lr(1e-3):0.000Epoch 1/1:  87%|████████▋ | 47/54 [19:11<02:45, 23.63s/it]
Epoch 1/1:  87%|████████▋ | 47/54 [19:11<02:45, 23.63s/it]Epoch 1/1:  87%|████████▋ | 47/54 [19:11<02:45, 23.63s/it]Epoch 1/1:  87%|████████▋ | 47/54 [19:05<02:45, 23.63s/it]step:47 - train/loss:0.158 - train/lr(1e-3):0.000Epoch 1/1:  89%|████████▉ | 48/54 [19:35<02:22, 23.77s/it]
Epoch 1/1:  89%|████████▉ | 48/54 [19:35<02:22, 23.77s/it]Epoch 1/1:  89%|████████▉ | 48/54 [19:35<02:22, 23.77s/it]Epoch 1/1:  89%|████████▉ | 48/54 [19:29<02:22, 23.77s/it]Epoch 1/1:  91%|█████████ | 49/54 [19:59<01:59, 23.94s/it]Epoch 1/1:  91%|█████████ | 49/54 [19:59<01:59, 23.94s/it]step:48 - train/loss:0.131 - train/lr(1e-3):0.000Epoch 1/1:  91%|█████████ | 49/54 [19:59<01:59, 23.94s/it]
Epoch 1/1:  91%|█████████ | 49/54 [19:53<01:59, 23.94s/it]Epoch 1/1:  93%|█████████▎| 50/54 [20:24<01:36, 24.07s/it]Epoch 1/1:  93%|█████████▎| 50/54 [20:24<01:36, 24.08s/it]Epoch 1/1:  93%|█████████▎| 50/54 [20:24<01:36, 24.08s/it]step:49 - train/loss:0.148 - train/lr(1e-3):0.000
Epoch 1/1:  93%|█████████▎| 50/54 [20:17<01:36, 24.08s/it]Epoch 1/1:  94%|█████████▍| 51/54 [20:48<01:12, 24.17s/it]Epoch 1/1:  94%|█████████▍| 51/54 [20:48<01:12, 24.17s/it]Epoch 1/1:  94%|█████████▍| 51/54 [20:48<01:12, 24.17s/it]step:50 - train/loss:0.133 - train/lr(1e-3):0.000
Epoch 1/1:  94%|█████████▍| 51/54 [20:42<01:12, 24.17s/it]Epoch 1/1:  96%|█████████▋| 52/54 [21:11<00:47, 23.87s/it]step:51 - train/loss:0.122 - train/lr(1e-3):0.000Epoch 1/1:  96%|█████████▋| 52/54 [21:11<00:47, 23.87s/it]
Epoch 1/1:  96%|█████████▋| 52/54 [21:05<00:47, 23.87s/it]Epoch 1/1:  96%|█████████▋| 52/54 [21:11<00:47, 23.87s/it]Epoch 1/1:  98%|█████████▊| 53/54 [21:36<00:23, 23.96s/it]Epoch 1/1:  98%|█████████▊| 53/54 [21:36<00:23, 23.96s/it]Epoch 1/1:  98%|█████████▊| 53/54 [21:36<00:23, 23.96s/it]step:52 - train/loss:0.112 - train/lr(1e-3):0.000
Epoch 1/1:  98%|█████████▊| 53/54 [21:29<00:23, 23.96s/it]step:53 - train/loss:0.132 - train/lr(1e-3):0.000
step:54 - val/loss:1.034
/iris/u/rypark/miniconda3/envs/verlc/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
Epoch 1/1:  98%|█████████▊| 53/54 [22:59<00:26, 26.03s/it]
Epoch 1/1:  98%|█████████▊| 53/54 [22:59<00:26, 26.03s/it]
Epoch 1/1:  98%|█████████▊| 53/54 [22:59<00:26, 26.03s/it]
[rank2]:[W304 01:04:02.092385098 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W304 01:04:02.092389937 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W304 01:04:02.098917138 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Epoch 1/1:  98%|█████████▊| 53/54 [22:55<00:25, 25.96s/it]
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     train/loss ██▇▆▄▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train/lr(1e-3) ▂▄▅▇██████▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:       val/loss ▁
wandb: 
wandb: Run summary:
wandb:     train/loss 0.13189
wandb: train/lr(1e-3) 0
wandb:       val/loss 1.03392
wandb: 
wandb: 🚀 View run llama-3.2-3b-instruct-l1l2-onlyapi at: https://wandb.ai/orangese/dense-tool-rewards/runs/f80bvhbs
wandb: ⭐️ View project at: https://wandb.ai/orangese/dense-tool-rewards
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250304_004013-f80bvhbs/logs
[rank0]:[W304 01:04:06.968569358 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
